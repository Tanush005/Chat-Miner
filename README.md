ChatMiner: A Production-Grade WhatsApp Chat Analysis Platform
An end-to-end data application engineered to perform high-speed, interactive analysis on large, unstructured WhatsApp chat histories. Deployed as a live web app on Streamlit Cloud.

<img width="2823" height="1529" alt="Screenshot 2025-07-22 200521" src="https://github.com/user-attachments/assets/203709fd-9a8b-4b67-9c72-936807b55d2a" />

ðŸŽ¯ The Problem
WhatsApp chats are a rich source of personal data, but they are notoriously difficult to analyze. Exported chat files are unstructured, and loading a large history with hundreds of thousands of messages into a standard script is slow and inefficient. Furthermore, finding specific information within these massive text files is nearly impossible with basic search tools.

ChatMiner was engineered to solve these problems by providing a production-grade platform that is fast, scalable, and powerful.

ðŸš€ Key Engineering Features
This project was built from the ground up with a focus on software engineering principles, not just data analysis. The architecture prioritizes performance, modularity, and robust information retrieval.

High-Performance Caching Pipeline:

Problem: Re-processing a large chat file on every visit is computationally expensive and leads to a poor user experience.

Solution: I architected a custom ETL pipeline that intelligently processes data. The system generates a unique hash for each uploaded file. On first load, it runs the ETL script and caches the cleaned, structured DataFrame as a highly efficient Parquet file.

Impact: This caching strategy reduces subsequent load times for the same file by over 95%, enabling near-instantaneous analysis for returning users.

Extensible OOP Architecture:

Problem: Monolithic scripts are difficult to maintain and extend.

Solution: I designed the entire analysis framework using Object-Oriented Programming (OOP) principles. Each component (data loading, analysis, visualization) is encapsulated in its own class, creating a "plug-and-play" system.

Impact: This modular design allows new features or analysis modules to be added with zero impact on existing code, demonstrating a commitment to writing clean, scalable, and maintainable software.

Advanced Information Retrieval System:

Problem: Standard text search (Ctrl+F) is insufficient for deep analysis.

Solution: I implemented a powerful, full-text search engine using the Whoosh library. The system creates a persistent search index from the chat data, allowing for complex, filtered queries.

Impact: Users can perform multi-filter searches (e.g., find all messages containing "meetup" from a specific user within a given date range), transforming the unstructured chat log into a queryable, high-performance data system.

ðŸ›ï¸ System & Data Flow Architecture
The application's data pipeline is designed for efficiency and scalability, ensuring a fast and responsive user experience.

graph TD
    A[User Uploads chat.txt] --> B{Generate File Hash};
    B --> C{Check for Cached Parquet File};
    C -- Yes --> D[Load from Cache];
    C -- No --> E[Run ETL Script];
    E --> F[Process & Clean Data];
    F --> G[Save to Parquet Cache];
    G --> D;
    D --> H[Interactive Dashboard (Streamlit)];
    
    subgraph "First-Time Load"
        E; F; G;
    end

    subgraph "Search Engine"
        F --> I[Create/Update Whoosh Index];
        J[User Search Query] --> K{Query Engine};
        I --> K;
        K --> H;
    end

    style A fill:#c8e6c9,stroke:#388e3c,stroke-width:2px
    style H fill:#2b3137,stroke:#fff,stroke-width:2px,color:#fff
    style I fill:#bbdefb,stroke:#1976d2,stroke-width:2px

ðŸ› ï¸ Tech Stack
Category

Technologies

Frontend & Deployment

Streamlit, Streamlit Cloud, Plotly

Backend & Architecture

Python, Object-Oriented Programming (OOP)

Data Pipeline & Storage

Pandas, NumPy, Apache Parquet, ETL

Search & Information Retrieval

Whoosh

âš™ï¸ Local Installation & Setup
Clone the Repository

git clone [Your GitHub Repository URL Here]
cd [your-repo-name]

Create and Activate a Virtual Environment

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

Install Dependencies

pip install -r requirements.txt

Run the Application

streamlit run app.py

ðŸ”® Future Enhancements
Asynchronous Data Processing: For extremely large files (>500MB), move the ETL and indexing processes to a background worker (e.g., using Celery/Redis) to prevent UI blocking.

Advanced NLP Analytics: Integrate NLP models (e.g., from Hugging Face) to perform sentiment analysis, topic modeling, and named entity recognition on the chat data.

Database Integration: For a multi-user environment, replace the file-based caching with a dedicated database system (like PostgreSQL or a document store) to manage data and indexes more robustly.

Containerization: Dockerize the application for consistent, isolated deployments and easier scaling on cloud platforms.
